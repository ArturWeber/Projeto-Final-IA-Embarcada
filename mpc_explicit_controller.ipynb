{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02f69ae",
   "metadata": {},
   "source": [
    "# Aproximador Explícito de Lei de Controle\n",
    "\n",
    "Abordagem baseada em aprendizado para aproximar a solução de um problema de Model Predictive Control (MPC), representado por um problema de otimização quadrático (QP Problem).\n",
    "\n",
    "O método desenvolvido a seguir desenvolve uma rede neural artificial (Multilayer Perceptron - MLP) que tenta encontrar o primal do problema QP através de aprendizado supervisionado, baseado no lagrangiano do problema e um conjunto de dados. As amostras de dados que compõem o dataset utilizado foram construídas pelos autores através do desenvolvimento de uma simulação do cenário de controle e um solver tradicional.\n",
    "\n",
    "Ao longo do trabalho, serão apresentados dois contextos de aprendizados:\n",
    "\n",
    "- #### Lagrangiano:\n",
    "Método inicial, proposto pelo artigo, que definia a função de perda a partir do lagrangiano do problema. No fim, essa foi preterida em razão da próxima, que foi a versão final utilizada.\n",
    "\n",
    "- #### Primal Restrita\n",
    "Método utilizado para substituir a função de perda lagrangiana. **Foi a versão final utilizada.**\n",
    "\n",
    "## Autores:\n",
    "* Artur\n",
    "* Carlos\n",
    "* Vicenzo D'Arezzo Zilio - 13671790"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5caac9",
   "metadata": {},
   "source": [
    "# 1. Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6134a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import math\n",
    "import zipfile\n",
    "import copy\n",
    "\n",
    "from itertools import product\n",
    "from typing import Callable, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from numpy.linalg import eigvals, norm\n",
    "\n",
    "import torch, platform\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.autograd import gradcheck\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282bf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './content/states_with_bounds.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58499041",
   "metadata": {},
   "source": [
    "## 1.1 Ambiente de Execução\n",
    "\n",
    "Contexto de hardware onde as funcionalidades do PyTorch serão executadas. Troque com base no cenário desejado.\n",
    "- MPS: framework GPU nativa dispositivos Apple\n",
    "- Cuda: framework GPU disponível no colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Torch:\", torch.__version__, \"| MPS:\", torch.mps.is_available(), \"| Py:\", platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da05054",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './content/states_with_bounds.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af70be0",
   "metadata": {},
   "source": [
    "# 2. Extração e Pré-processaemento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6064bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(DATA_PATH, 'r') as zf:\n",
    "    print(\"Conteúdo:\", zf.namelist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fc375",
   "metadata": {},
   "source": [
    "## 2.1 Carregamento de dados\n",
    "Com base na definição das bases de dados gerada em simulação, cujo header pode ser visualizado pela célula anterior, é definida uma classe especializada para armazenar e validar as informações retidas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b5c42",
   "metadata": {},
   "source": [
    "### 2.1.1 Problema Lagrangiano\n",
    "São aplicadas diversas transformações de pré-processamento, principalmente em razão da instabilidade e dimensionalidade do problema.\n",
    "Nessa versão foi realizada a visualização dos dados, portanto, aqui também se encontram os métodos de visualização.\n",
    "\n",
    "- Tikhonov na matriz P - para garantir semi-positividade na matriz\n",
    "- Normalização espectral nas matrizes - para melhorar condicionalidade\n",
    "- Normalização por z-scalling no estado inicial, primal e duais - para melhorar desempenho do modelo e garantir equilíbrio numérico\n",
    "- Escalonamento do problema a partir do primal - para reduzir condicionalidade da matriz P durante o cálculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Init_MPC_Problem:\n",
    "\n",
    "    def __init__(self, np_raw_path):\n",
    "\n",
    "      raw = np.load(DATA_PATH)\n",
    "\n",
    "      # Dimensions\n",
    "      self.n_states = int(raw[\"Nx\"])\n",
    "      self.n_actuations = int(raw[\"Nu\"])\n",
    "      self.n_horizon = int(raw[\"Nh\"])\n",
    "      self.dim_z = (self.n_states + self.n_actuations) * self.n_horizon\n",
    "\n",
    "      # Constraints\n",
    "      self.n_constraints =  int(raw[\"n_con\"])\n",
    "      self.n_eq_constraints = int(raw[\"n_eq\"])\n",
    "      self.n_ineq_constraints = int(raw[\"n_in\"])\n",
    "      \n",
    "      # Iterating through data to extract each instance:\n",
    "      # -> Decision variables, Duals and Bounds\n",
    "\n",
    "      self.input_data = raw[\"data\"].astype(np.float32)\n",
    "      it_pos = 0\n",
    "\n",
    "      self.state_list = self.input_data[:, it_pos: it_pos + self.n_states]\n",
    "      it_pos += self.n_states\n",
    "      \n",
    "      self.z_list = self.input_data[:, it_pos : it_pos + self.dim_z]\n",
    "      it_pos += self.dim_z\n",
    "\n",
    "      self.raw_dual = self.input_data[:, it_pos : it_pos + self.n_constraints]\n",
    "      it_pos += self.n_constraints\n",
    "\n",
    "      self.lower_bounds = self.input_data[:, it_pos : it_pos + self.n_constraints]\n",
    "      it_pos += self.n_constraints\n",
    "\n",
    "      self.upper_bounds = self.input_data[:, it_pos : it_pos + self.n_constraints]\n",
    "\n",
    "      # Quadratic Programming Matrices\n",
    "      self.P = raw[\"P\"].astype(np.float64)\n",
    "      self.q = raw[\"q\"].astype(np.float64)\n",
    "      self.C = raw[\"C\"].astype(np.float64)\n",
    "      self.U = raw[\"U\"].astype(np.float64)\n",
    "\n",
    "      # Constraints Matrix\n",
    "      self.D = np.concatenate((self.C,  self.U), axis=0)\n",
    "\n",
    "      # Duals Extraction\n",
    "      self.lamb = np.maximum(-self.raw_dual, 0)\n",
    "      self.nu = np.maximum(self.raw_dual, 0)\n",
    "\n",
    "      # Dimension Check\n",
    "      assert self.P.shape == (self.dim_z, self.dim_z), f\"P must be [{self.dim_z},{self.dim_z}], got {self.P.shape}\"\n",
    "      assert self.q.shape == (self.dim_z,), f\"q must be [{self.dim_z}], got {self.q.shape}\"\n",
    "      assert self.C.shape == (self.n_eq_constraints, self.dim_z), f\"C must be [{self.n_eq_constraints},{self.dim_z}], got {self.Q.shape}\"\n",
    "      assert self.U.shape == (self.n_ineq_constraints, self.dim_z), f\"U must be [{self.n_ineq_constraints},{self.dim_z}], got {self.U.shape}\"\n",
    "\n",
    "      N_samples = self.input_data.shape[0]\n",
    "      assert self.state_list.shape[0] == N_samples, f\"state_list must have {N_samples} samples, got {self.state_list.shape[0]}\"\n",
    "      assert self.z_list.shape[0] == N_samples, f\"z_list must have {N_samples} samples, got {self.z_list.shape[0]}\"\n",
    "      assert self.raw_dual.shape[0] == N_samples, f\"raw_dual must have {N_samples} samples, got {self.raw_dual.shape[0]}\"\n",
    "      assert self.upper_bounds.shape[0] == N_samples, f\"upper_bounds must have {N_samples} samples, got {self.upper_bounds.shape[0]}\"\n",
    "      assert self.lower_bounds.shape[0] == N_samples, f\"lower_bounds must have {N_samples} samples, got {self.lower_bounds.shape[0]}\"\n",
    "      \n",
    "      assert self.state_list.shape[1] == self.n_states, f\"state_list must have {self.n_states} features, got {self.state_list.shape[1]}\"\n",
    "      assert self.z_list.shape[1] == self.dim_z, f\"z_list must have {self.dim_z} features, got {self.z_list.shape[1]}\"\n",
    "      assert self.raw_dual.shape[1] == self.n_constraints, f\"raw_lag must have {self.n_constraints} features, got {self.raw_lag.shape[1]}\"\n",
    "      assert self.upper_bounds.shape[1] == self.n_constraints, f\"upper_bounds must have {self.n_constraints} features, got {self.upper_bounds.shape[1]}\"\n",
    "      assert self.lower_bounds.shape[1] == self.n_constraints, f\"lower_bounds must have {self.n_constraints} features, got {self.lower_bounds.shape[1]}\"\n",
    "\n",
    "      total_expected_cols = self.n_states + self.dim_z  + self.n_constraints * 3\n",
    "      assert self.input_data.shape[1] == total_expected_cols, f\"Total input columns must be {total_expected_cols}, got {self.input_data.shape[1]}. Check slicing logic.\"\n",
    "      print(f\"[CHECK] - Dimensions and Sampling Ok\")\n",
    "\n",
    "      # P nature checking\n",
    "      eigvals = np.linalg.eigvalsh(self.P)\n",
    "      assert np.all(eigvals >= -1e-8), f\"P not positive semidefinite, min eig = {eigvals.min()}\"\n",
    "      print(f\"[CHECK] - P Matrices are Positive Semidefinite\")\n",
    "\n",
    "    def check_P_matrix_condition(self):\n",
    "      \"\"\"\n",
    "      The condition number gives an indication of the tightness\n",
    "      of the bounds on the curvature of f, and therefore\n",
    "      of the difficulty to optimize it: the bigger the value of\n",
    "      κ is, the slowest is the convergence of the algorithm\n",
    "      \"\"\"\n",
    "      condition_number = np.linalg.cond(self.P)\n",
    "      print(f\"[CHECK] - Condition Number of P: {condition_number}\")\n",
    "      if condition_number > 1e6:\n",
    "        print(f\"[WARNING] - Condition Number of P matrix is too high, this may lead to numerical instability\")\n",
    "\n",
    "    def apply_pre_processing(self):\n",
    "\n",
    "      # Condition transformations\n",
    "      self.apply_tikhonov_regularization()\n",
    "      self.normalizating_qp_matrices()\n",
    "\n",
    "      # Distribuition transformation\n",
    "      self.apply_state_standarization()\n",
    "      self.apply_decision_variable_standarization()\n",
    "      self.apply_duals_scalling()\n",
    "      self.decision_variable_scalling()\n",
    "      pass\n",
    "\n",
    "    def apply_state_standarization(self):\n",
    "      \"\"\"\n",
    "      x' = (x - x_mean) / x_std\n",
    "      \"\"\"\n",
    "      self.state_mean = self.state_list.mean(axis=0)\n",
    "      self.state_std = self.state_list.std(axis=0)\n",
    "      self.state_list = (self.state_list - self.state_mean) / self.state_std\n",
    "      print(f\"[INFO] - State Standarization applied\")\n",
    "\n",
    "    def apply_decision_variable_standarization(self):\n",
    "      \"\"\"\n",
    "      z' = (z - z_mean) / z_std\n",
    "      \"\"\"\n",
    "\n",
    "      self.z_mean = self.z_list.mean(axis=0)\n",
    "      self.z_std = self.z_list.std(axis=0)\n",
    "      self.z_list = (self.z_list - self.z_mean) / self.z_std\n",
    "\n",
    "      # This process must be applied to the constraints ineq:\n",
    "      # a <= D z' <= b -> a <= D(mean) + (D @ diag(std)) z <= b\n",
    "      # a' = a - D(mean) (same for b)\n",
    "      # D' = D @ diag(std)\n",
    "\n",
    "      self.lower_bounds = self.lower_bounds - self.D @ self.z_mean # Old D\n",
    "      self.upper_bounds = self.upper_bounds - self.D @ self.z_mean # Old D\n",
    "      self.D = self.D @ np.diag(self.z_std)\n",
    "      \n",
    "      # This process must be applie to the objective function:\n",
    "      # P' = diag(std) @ P @ diag(std)\n",
    "      # q = diag(std) @ P @ mean + diag(std) @ s\n",
    "\n",
    "      self.q = np.diag(self.z_std) @ (self.P @ self.z_mean + self.q) # Dld P\n",
    "      self.P = np.diag(self.z_std) @ self.P @ np.diag(self.z_std)\n",
    "\n",
    "      print(f\"[INFO] - Decision Variable Standarization applied\")\n",
    "\n",
    "    def apply_duals_scalling(self):\n",
    "      \"\"\"\n",
    "      nu_std = (nu - nu_mean) / nu_std\n",
    "      lambda_std = (lambda - lambda_mean) / lambda_std\n",
    "      \"\"\"\n",
    "      self.nu = self.nu / self.nu.std(axis=0) + 1e-6\n",
    "      self.lamb = self.lamb / self.lamb.std(axis=0) + 1e-6\n",
    "      print(f\"[INFO] - Duals Scalling by Std Dev applied\")\n",
    "\n",
    "    def apply_tikhonov_regularization(self, gamma=1e-5):\n",
    "      \"\"\"\n",
    "      P_reg = P + gamma * I.\n",
    "      Ensure positive definite and positive semidefinite nature of P and improve\n",
    "      numerical stability by decreasing P condition number.\n",
    "      \"\"\"\n",
    "      # Adicionar o termo de regularização à matriz P\n",
    "      self.P = self.P + gamma * np.eye(self.P.shape[0], dtype=np.float64)\n",
    "      print(f\"[INFO] - Tikhonov Regularization applied = {gamma}\")\n",
    "\n",
    "    def normalizating_qp_matrices(self):\n",
    "      \"\"\"\n",
    "        -> Normalizing P by its spectral norm\n",
    "      \"\"\"\n",
    "      spec_norm_P = np.linalg.norm(self.P, ord=2)\n",
    "\n",
    "      self.P = self.P / spec_norm_P\n",
    "      self.q = self.q / spec_norm_P\n",
    "      \n",
    "      print(f\"[INFO] - Normalizing P and q by P's spectral norm = {spec_norm_P}\")\n",
    "\n",
    "    def decision_variable_scalling(self):\n",
    "      \"\"\"\n",
    "      Maps the optimization problem from z -> z', where\n",
    "      z' = D^-1 z and D is calculated in order to reduce\n",
    "      the condition number of P.\n",
    "\n",
    "      D (or D^-1) must be applied to all the operations\n",
    "      in relation to z\n",
    "      \"\"\"\n",
    "\n",
    "      # Calculating scale:\n",
    "      P_diag = np.diag(self.P)\n",
    "      scale = np.sqrt(P_diag)\n",
    "      scale[scale < 1e-9] = 1.0\n",
    "\n",
    "      # Applying scale:\n",
    "      self.z_scalling = np.diag(1.0 / scale)\n",
    "      self.z_inv_scalling = 1/np.diag(self.z_scalling)\n",
    "\n",
    "      self.P = self.z_scalling.T @ self.P @ self.z_scalling\n",
    "      self.q = self.z_scalling.T @ self.q\n",
    "      self.D = self.D @ self.z_scalling\n",
    "\n",
    "      self.z_list = self.z_inv_scalling * self.z_list\n",
    "\n",
    "      print(f\"[INFO] - Decision Variable Scaling applied\")\n",
    "\n",
    "    def visualize_bounds(self):\n",
    "      x = self.lower_bounds\n",
    "      z = self.upper_bounds\n",
    "\n",
    "      plt.figure(figsize=(10, 4))\n",
    "      plt.boxplot(x, showfliers=False)\n",
    "      plt.title(\"Lower Bounds (z_inf)\")\n",
    "      plt.xlabel(\"Dimension\")\n",
    "      plt.ylabel(\"Value\")\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(10, 4))\n",
    "      plt.boxplot(z, showfliers=False)\n",
    "      plt.title(\"Upper Bounds (z_sup)\")\n",
    "      plt.xlabel(\"Dimension\")\n",
    "      plt.ylabel(\"Value\")\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "    def visualize_duals(self):\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.boxplot(self.lamb, showfliers=False)\n",
    "        plt.title(\"Lambda (lower bound dual)\")\n",
    "        plt.xlabel(\"Dimension\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        lamb_mean = self.lamb.mean(axis=0)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.bar(range(len(lamb_mean)), lamb_mean, capsize=5)\n",
    "        plt.title(\"Lambda – Mean value per dimension\")\n",
    "        plt.xlabel(\"Dimension\")\n",
    "        plt.ylabel(\"Mean\")\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.boxplot(self.nu, showfliers=False)\n",
    "        plt.title(\"Nu (upper bound dual)\")\n",
    "        plt.xlabel(\"Dimension\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        nu_mean = self.nu.mean(axis=0)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.bar(range(len(nu_mean)), nu_mean, capsize=5)\n",
    "        plt.title(\"Nu – Mean value per dimension\")\n",
    "        plt.xlabel(\"Dimension\")\n",
    "        plt.ylabel(\"Mean\")\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def visualize_decision_vars(self):\n",
    "      x = self.state_list\n",
    "      z = self.z_list\n",
    "\n",
    "      def summarize_array(arr, name):\n",
    "        df = pd.DataFrame(arr, columns=[f\"{name}_{i}\" for i in range(arr.shape[1])])\n",
    "        desc = df.describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]).T\n",
    "        print(f\"\\nResumo estatístico de {name}:\")\n",
    "        print(desc[[\"mean\", \"std\", \"min\", \"1%\", \"5%\", \"50%\", \"95%\", \"99%\", \"max\"]])\n",
    "        return desc\n",
    "\n",
    "      summarize_array(x, \"x\")\n",
    "      summarize_array(z, \"z\")\n",
    "\n",
    "      plt.figure(figsize=(10, 4))\n",
    "      plt.boxplot(x, showfliers=False)\n",
    "      plt.title(\"State Distribuition (x)\")\n",
    "      plt.xlabel(\"Dimension\")\n",
    "      plt.ylabel(\"Value\")\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(10, 4))\n",
    "      plt.boxplot(z, showfliers=False)\n",
    "      plt.title(\"Decision Variable Distribuition (z)\")\n",
    "      plt.xlabel(\"Dimension\")\n",
    "      plt.ylabel(\"Value\")\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "    def visualize_C_and_U(self):\n",
    "\n",
    "      def _visualize_matrix(M, title_prefix=\"Matrix\"):\n",
    "        diag = np.diag(M)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "        # 1. Valores da diagonal\n",
    "        axs[0].plot(diag, marker='o')\n",
    "        axs[0].set_title(f\"{title_prefix} – diagonal\")\n",
    "        axs[0].set_xlabel(\"Index\")\n",
    "        axs[0].set_ylabel(\"Value\")\n",
    "\n",
    "        # 2. Heatmap da matriz\n",
    "        im = axs[1].imshow(M, aspect='auto', cmap='viridis')\n",
    "        axs[1].set_title(f\"{title_prefix} – heatmap\")\n",
    "        plt.colorbar(im, ax=axs[1])\n",
    "\n",
    "        # 3. Heatmap absoluto (concentração de valores)\n",
    "        im2 = axs[2].imshow(np.abs(M), aspect='auto', cmap='inferno')\n",
    "        axs[2].set_title(f\"{title_prefix} – magnitude |M|\")\n",
    "        plt.colorbar(im2, ax=axs[2])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "      print(\"=== C MATRIX ===\")\n",
    "      _visualize_matrix(self.C, title_prefix=\"C\")\n",
    "\n",
    "      print(\"=== U MATRIX ===\")\n",
    "      _visualize_matrix(self.U, title_prefix=\"U\")\n",
    "\n",
    "      print(\"=== D MATRIX ===\")\n",
    "      _visualize_matrix(self.D, title_prefix=\"D\")\n",
    "\n",
    "    def visualize_P_and_q(self):\n",
    "\n",
    "      P = self.P\n",
    "      q = self.q\n",
    "      P = P if not issparse(P) else P.tocsr()\n",
    "      q = q.reshape(-1)\n",
    "\n",
    "      # ---- 1. SPY Plot ----\n",
    "      plt.figure(figsize=(6, 6))\n",
    "      if issparse(P):\n",
    "        plt.spy(P, markersize=1)\n",
    "      else:\n",
    "        plt.imshow(P != 0, aspect=\"auto\", cmap=\"gray_r\")\n",
    "      plt.title(\"Sparsity Pattern of P\")\n",
    "      plt.show()\n",
    "\n",
    "      # ---- 2. Heatmap for dense or sampled ----\n",
    "      if not issparse(P) and P.shape[0] <= 3000:\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(P, aspect='auto')\n",
    "        plt.title(\"Heatmap of P values\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "      # ---- 3. Histogram of values ----\n",
    "      P_vals = P.data if issparse(P) else P.flatten()\n",
    "      plt.figure(figsize=(6,4))\n",
    "      plt.hist(P_vals, bins=100)\n",
    "      plt.title(\"Distribution of P entries\")\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(6,4))\n",
    "      plt.hist(q, bins=50)\n",
    "      plt.title(\"Distribution of q entries\")\n",
    "      plt.show()\n",
    "\n",
    "      # ---- Boxplot & Violinplot for q ----\n",
    "      plt.figure(figsize=(6,4))\n",
    "      plt.boxplot(q)\n",
    "      plt.title(\"Boxplot of q values\")\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(6,4))\n",
    "      plt.violinplot(q, showmeans=True)\n",
    "      plt.title(\"Violin Plot of q values\")\n",
    "      plt.show()\n",
    "\n",
    "      # ---- 4. Symmetry check ----\n",
    "      if not issparse(P):\n",
    "        sym_error = norm(P - P.T)\n",
    "        print(f\"Symmetry error ||P - P^T||: {sym_error}\")\n",
    "\n",
    "      # ---- 5. Eigenvalues (smallest few) ----\n",
    "      print(\"Estimating smallest eigenvalues of P...\")\n",
    "      try:\n",
    "        if issparse(P):\n",
    "          evals = eigsh(P, k=6, which='SA')[0]\n",
    "        else:\n",
    "          evals = np.sort(eigvals(P).real)[:6]\n",
    "        print(\"Smallest eigenvalues:\", evals)\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(evals, bins=10)\n",
    "        plt.title(\"Histogram of smallest eigenvalues of P\")\n",
    "        plt.show()\n",
    "\n",
    "        # ---- Boxplot & Violinplot for eigenvalues ----\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.boxplot(evals)\n",
    "        plt.title(\"Boxplot of smallest eigenvalues of P\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.violinplot(evals, showmeans=True)\n",
    "        plt.title(\"Violin Plot of smallest eigenvalues of P\")\n",
    "        plt.show()\n",
    "        plt.title(\"Histogram of smallest eigenvalues of P\")\n",
    "        plt.show()\n",
    "\n",
    "      except Exception as e:\n",
    "        print(\"Eigenvalue analysis failed:\", e)\n",
    "\n",
    "      # ---- 6. Row norm profile ----\n",
    "      if not issparse(P):\n",
    "        row_norms = np.linalg.norm(P, axis=1)\n",
    "      else:\n",
    "        row_norms = np.sqrt(P.multiply(P).sum(axis=1)).A.flatten()\n",
    "\n",
    "\n",
    "      plt.figure(figsize=(8,4))\n",
    "      plt.plot(row_norms)\n",
    "      plt.title(\"Row Norm Profile of P\")\n",
    "      plt.show()\n",
    "\n",
    "      # ---- 7. Inspect q structure ----\n",
    "      plt.figure(figsize=(8,4))\n",
    "      plt.plot(q)\n",
    "      plt.title(\"q vector profile\")\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72708c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC = Init_MPC_Problem(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db744eb",
   "metadata": {},
   "source": [
    "#### 2.1.1.1 Visualização Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d210d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.visualize_decision_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19418976",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.visualize_duals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.visualize_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.visualize_P_and_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.visualize_C_and_U()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a53e1f4",
   "metadata": {},
   "source": [
    "#### 2.1.1.2 Aplicação do Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4df1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.check_P_matrix_condition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53028e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.apply_pre_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.check_P_matrix_condition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.visualize_decision_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d231373",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_MPC.visualize_duals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1601dc",
   "metadata": {},
   "source": [
    "### 2.1.2 Problema Final\n",
    "Não são aplicadas as transformações complexas em razão da não utilização das matrizes instáveis do problema na função de perda.\n",
    "São realizadas:\n",
    "\n",
    "- Normalização z-scalling nos estados iniciais e no primal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0863a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPC_Problem:\n",
    "\n",
    "  def __init__(self, np_raw_path):\n",
    "\n",
    "    raw = np.load(DATA_PATH)\n",
    "\n",
    "    # Dimensions\n",
    "    self.n_states = int(raw[\"Nx\"])\n",
    "    self.n_actuations = int(raw[\"Nu\"])\n",
    "    self.n_horizon = int(raw[\"Nh\"])\n",
    "    self.dim_z = (self.n_states + self.n_actuations) * self.n_horizon\n",
    "\n",
    "    # Constraints\n",
    "    self.n_constraints =  int(raw[\"n_con\"])\n",
    "    self.n_eq_constraints = int(raw[\"n_eq\"])\n",
    "    self.n_ineq_constraints = int(raw[\"n_in\"])\n",
    "    \n",
    "    # Iterating through data to extract each instance:\n",
    "    # -> Decision variables, Duals and Bounds\n",
    "\n",
    "    self.input_data = raw[\"data\"].astype(np.float32)\n",
    "    it_pos = 0\n",
    "\n",
    "    self.state_list = self.input_data[:, it_pos: it_pos + self.n_states]\n",
    "    it_pos += self.n_states\n",
    "    \n",
    "    self.z_list = self.input_data[:, it_pos : it_pos + self.dim_z]\n",
    "    it_pos += self.dim_z\n",
    "\n",
    "    self.raw_dual = self.input_data[:, it_pos : it_pos + self.n_constraints]\n",
    "    it_pos += self.n_constraints\n",
    "\n",
    "    self.lower_bounds = self.input_data[:, it_pos : it_pos + self.n_constraints]\n",
    "    it_pos += self.n_constraints\n",
    "\n",
    "    self.upper_bounds = self.input_data[:, it_pos : it_pos + self.n_constraints]\n",
    "\n",
    "    # Quadratic Programming Matrices\n",
    "    self.P = raw[\"P\"].astype(np.float64)\n",
    "    self.q = raw[\"q\"].astype(np.float64)\n",
    "    self.C = raw[\"C\"].astype(np.float64)\n",
    "    self.U = raw[\"U\"].astype(np.float64)\n",
    "\n",
    "    # Constraints Matrix\n",
    "    self.D = np.concatenate((self.C,  self.U), axis=0)\n",
    "\n",
    "    # Duals Extraction\n",
    "    self.lamb = np.maximum(-self.raw_dual, 0)\n",
    "    self.nu = np.maximum(self.raw_dual, 0)\n",
    "\n",
    "    # Dimension Check\n",
    "    assert self.P.shape == (self.dim_z, self.dim_z), f\"P must be [{self.dim_z},{self.dim_z}], got {self.P.shape}\"\n",
    "    assert self.q.shape == (self.dim_z,), f\"q must be [{self.dim_z}], got {self.q.shape}\"\n",
    "    assert self.C.shape == (self.n_eq_constraints, self.dim_z), f\"C must be [{self.n_eq_constraints},{self.dim_z}], got {self.Q.shape}\"\n",
    "    assert self.U.shape == (self.n_ineq_constraints, self.dim_z), f\"U must be [{self.n_ineq_constraints},{self.dim_z}], got {self.U.shape}\"\n",
    "\n",
    "    N_samples = self.input_data.shape[0]\n",
    "    assert self.state_list.shape[0] == N_samples, f\"state_list must have {N_samples} samples, got {self.state_list.shape[0]}\"\n",
    "    assert self.z_list.shape[0] == N_samples, f\"z_list must have {N_samples} samples, got {self.z_list.shape[0]}\"\n",
    "    assert self.raw_dual.shape[0] == N_samples, f\"raw_dual must have {N_samples} samples, got {self.raw_dual.shape[0]}\"\n",
    "    assert self.upper_bounds.shape[0] == N_samples, f\"upper_bounds must have {N_samples} samples, got {self.upper_bounds.shape[0]}\"\n",
    "    assert self.lower_bounds.shape[0] == N_samples, f\"lower_bounds must have {N_samples} samples, got {self.lower_bounds.shape[0]}\"\n",
    "    \n",
    "    assert self.state_list.shape[1] == self.n_states, f\"state_list must have {self.n_states} features, got {self.state_list.shape[1]}\"\n",
    "    assert self.z_list.shape[1] == self.dim_z, f\"z_list must have {self.dim_z} features, got {self.z_list.shape[1]}\"\n",
    "    assert self.raw_dual.shape[1] == self.n_constraints, f\"raw_lag must have {self.n_constraints} features, got {self.raw_lag.shape[1]}\"\n",
    "    assert self.upper_bounds.shape[1] == self.n_constraints, f\"upper_bounds must have {self.n_constraints} features, got {self.upper_bounds.shape[1]}\"\n",
    "    assert self.lower_bounds.shape[1] == self.n_constraints, f\"lower_bounds must have {self.n_constraints} features, got {self.lower_bounds.shape[1]}\"\n",
    "\n",
    "    total_expected_cols = self.n_states + self.dim_z  + self.n_constraints * 3\n",
    "    assert self.input_data.shape[1] == total_expected_cols, f\"Total input columns must be {total_expected_cols}, got {self.input_data.shape[1]}. Check slicing logic.\"\n",
    "    print(f\"[CHECK] - Dimensions and Sampling Ok\")\n",
    "\n",
    "    # P nature checking\n",
    "    eigvals = np.linalg.eigvalsh(self.P)\n",
    "    assert np.all(eigvals >= -1e-8), f\"P not positive semidefinite, min eig = {eigvals.min()}\"\n",
    "    print(f\"[CHECK] - P Matrices are Positive Semidefinite\")\n",
    "    \n",
    "  def apply_primal_std(self):\n",
    "    \"\"\"\n",
    "    t(z) -> z' | z' = z - mean / std\n",
    "    \"\"\"\n",
    "    self.z_mean = self.z_list.mean(axis=0)\n",
    "    self.z_std = self.z_list.std(axis=0)\n",
    "    self.z_list = (self.z_list - self.z_mean) / self.z_std\n",
    "\n",
    "    # This process must be applied to the constraints ineq:\n",
    "    # a <= D z' <= b -> a <= D(mean) + (D @ diag(std)) z <= b\n",
    "    # a' = a - D(mean) (same for b)\n",
    "    # D' = D @ diag(std)\n",
    "\n",
    "    self.lower_bounds = self.lower_bounds - self.D @ self.z_mean # Old D\n",
    "    self.upper_bounds = self.upper_bounds - self.D @ self.z_mean # Old D\n",
    "    self.D = self.D @ np.diag(self.z_std)\n",
    "    \n",
    "    # Ignoring P and q propagations of t\n",
    "  \n",
    "  def apply_state_std(self):\n",
    "    \"\"\"\n",
    "    t(z) -> z' | z' = z - mean / std\n",
    "    \"\"\"\n",
    "\n",
    "    self.state_mean = self.state_list.mean(axis=0)\n",
    "    self.state_std = self.state_list.std(axis=0)\n",
    "    self.state_list = (self.state_list - self.state_mean) / self.state_std\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46dd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MPC = MPC_Problem(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc83160",
   "metadata": {},
   "source": [
    "#### 2.1.2.1 Aplicação do Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MPC.apply_primal_std()\n",
    "MPC.apply_state_std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96deeef4",
   "metadata": {},
   "source": [
    "## 2.2 Definição do Conjunto de Dados\n",
    "Tradução dos dados pré-processados a uma base de dados PyTorch, com o objetivo de propiciar o treinamento da rede posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPC_DATASET(Dataset):\n",
    "\n",
    "  def __init__(self, MPC):\n",
    "\n",
    "    self.states = MPC.state_list\n",
    "    self.decision_variables = MPC.z_list\n",
    "    self.nu_lagrangians = MPC.nu\n",
    "    self.lambda_lagrangians = MPC.lamb\n",
    "    self.lower_bounds = MPC.lower_bounds\n",
    "    self.upper_bounds = MPC.upper_bounds\n",
    "\n",
    "    self.D = torch.from_numpy(MPC.D).float()\n",
    "\n",
    "    # Pre-processing parameters\n",
    "\n",
    "    # print(f\"[CHECK] States list dimension: {self.states.shape}\")\n",
    "    # print(f\"[CHECK] Decision Variables lis dimension: {self.decision_variables.shape}\")\n",
    "    # print(f\"[CHECK] Nu Lagrangians dimension: {self.nu_lagrangians.shape}\")\n",
    "    # print(f\"[CHECK] Lambda Lagrangians dimension: {self.lambda_lagrangians.shape}\")\n",
    "\n",
    "    # print(f\"[CHECK] States list element list[i]: {self.states[0].shape}\")\n",
    "    # print(f\"[CHECK] Decision Variables list element list[i]: {self.decision_variables[0].shape}\")\n",
    "    # print(f\"[CHECK] Nu Lagrangians element list[i]: {self.nu_lagrangians[0].shape}\")\n",
    "    # print(f\"[CHECK] Lambda Lagrangians element list[i]: {self.lambda_lagrangians[0].shape}\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.states.shape[0]\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "\n",
    "    \"\"\"\n",
    "    Returning:\n",
    "    -> x std\n",
    "    -> z std\n",
    "    -> z_inf\n",
    "    -> z_sup\n",
    "    \"\"\"\n",
    "    x = self.states[i]\n",
    "    z = self.decision_variables[i]\n",
    "    lb = self.lower_bounds[i]\n",
    "    ub = self.upper_bounds[i]\n",
    "\n",
    "    return {\n",
    "        \"x\": torch.from_numpy(x).float(),\n",
    "        \"z\": torch.from_numpy(z).float(),\n",
    "        \"lb\": torch.from_numpy(lb).float(),\n",
    "        \"ub\": torch.from_numpy(ub).float()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bbcd31",
   "metadata": {},
   "source": [
    "# 3. Função de Perda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5aa89a",
   "metadata": {},
   "source": [
    "## 3.1 Função de Perda Lagrangiana\n",
    "\n",
    "Definida como a média do quadrado da diferença do valor do Lagrangiano com o primal estimado e o primal ótimo. Utilizando o Lagrangiano aumentado, é possível incluir tanto a influência do valor da função objetivo - através do primal - quanto da contemplação das restrições - a partir de cada dual e dos termos inclusos na versão aumentada do lagrangiano.\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) :=\n",
    "\\sum_{i=1}^{}\n",
    "\\Big(\n",
    "    ℒ\\big(\\tilde{\\pi}(x_i \\mid \\theta), \\nu_i^{\\ast}, \\lambda_i^{\\ast} \\mid x_i\\big)\n",
    "    - ℒ\\big(z_i^{\\ast}, \\nu_i^{\\ast}, \\lambda_i^{\\ast} \\mid x_i\\big)\n",
    "\\Big)^2\n",
    "$$\n",
    "\n",
    "### Cálculo do Lagrangiano\n",
    "Feito a partir da interpretação das matrizes C e U como matrizes das restrições de igualdade e desigualdade em zero.\n",
    "\n",
    "$$\n",
    "ℒ(z, \\nu, \\lambda) = \\frac{1}{2}z^t\\mathbf{P}z + \\mathbf{q}^Tz + \\lambda(\\mathbf{D}z - z_{inf}) + \\nu^t(z_{sup} - \\mathbf{D}z) + \\frac{\\rho}{2}(\\max{0, z_{inf} - \\mathbf{D}z}) + \\frac{\\rho}{2}(\\max{0, \\mathbf{D}z} - z_{sup})\n",
    "$$\n",
    "\n",
    "### Problema Obtidos\n",
    "Inúmero problemas foram obtidos com essa função. Foram realizados os testes:\n",
    "- Correlação Valor de Loss e Gradiente para valores aleatórios (OK)\n",
    "- Convergência para caso de Overfit - capacidade de aprender (OK)\n",
    "- Distribuição de Probabilidade de Valor de Loss para valores aleatórios (Não OK)\n",
    "    - Valor esperaldo alto mas estável\n",
    "    - Valor explodindo frequentemente em termos quadráticos, duais e do aumentado\n",
    "    - OBS: gráficos no relatório\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3eebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedLagrangianLoss(nn.Module):\n",
    "    def __init__(self, P, q, D, rho):\n",
    "        super().__init__()\n",
    "\n",
    "        if q.dim() == 1:\n",
    "            q = q.unsqueeze(-1)  # (n,1)\n",
    "\n",
    "        self.register_buffer(\"P\", P.clone().detach().float())   # (n,n)\n",
    "        self.register_buffer(\"q\", q.clone().detach().float())   # (n,1)\n",
    "        self.register_buffer(\"D\", D.clone().detach().float())   # (m,n)\n",
    "        self.rho = float(rho)\n",
    "\n",
    "        self.quadratic_list = []\n",
    "        self.linear_list = []\n",
    "        self.lambda_list = []\n",
    "        self.nu_list = []\n",
    "        self.aug_inf_list = []\n",
    "        self.aug_sup_list = []\n",
    "\n",
    "        self.quadratic_list_opt = []\n",
    "        self.linear_list_opt = []\n",
    "        self.lambda_list_opt = []\n",
    "        self.nu_list_opt = []\n",
    "        self.aug_inf_list_opt = []\n",
    "        self.aug_sup_list_opt = []\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Plota lado a lado:\n",
    "        - valores não ótimos (listas normais)\n",
    "        - valores ótimos (listas *_opt)\n",
    "        \"\"\"\n",
    "\n",
    "        # Nome das listas e títulos\n",
    "        items = [\n",
    "            (\"quadratic_list\", \"quadratic_list_opt\", \"Quadratic Term\"),\n",
    "            (\"linear_list\", \"linear_list_opt\", \"Linear Term\"),\n",
    "            (\"lambda_list\", \"lambda_list_opt\", \"Lambda Term\"),\n",
    "            (\"nu_list\", \"nu_list_opt\", \"Nu Term\"),\n",
    "            (\"aug_inf_list\", \"aug_inf_list_opt\", \"Augmented (Inf)\"),\n",
    "            (\"aug_sup_list\", \"aug_sup_list_opt\", \"Augmented (Sup)\"),\n",
    "        ]\n",
    "\n",
    "        for attr_non_opt, attr_opt, title in items:\n",
    "\n",
    "            # --- Recupera as listas ---\n",
    "            list_non_opt = getattr(self, attr_non_opt)\n",
    "            list_opt = getattr(self, attr_opt)\n",
    "\n",
    "            # Converter tudo para numpy arrays 1D\n",
    "            def to_numpy_list(L):\n",
    "                processed = []\n",
    "                for x in L:\n",
    "                    if hasattr(x, \"detach\"):   # Tensor\n",
    "                        x = x.detach().cpu().numpy()\n",
    "                    if isinstance(x, np.ndarray):\n",
    "                        processed.append(x.flatten())\n",
    "                    else:\n",
    "                        processed.append(np.array([float(x)]))\n",
    "                if len(processed) == 0:\n",
    "                    return np.array([])\n",
    "                return np.concatenate(processed)\n",
    "\n",
    "            arr_non_opt = to_numpy_list(list_non_opt)\n",
    "            arr_opt = to_numpy_list(list_opt)\n",
    "\n",
    "            # --- Criar figura com dois subplots ---\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            fig.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "            # Subplot da versão não ótima\n",
    "            axes[0].plot(arr_non_opt)\n",
    "            axes[0].set_title(\"Pred (não ótimo)\")\n",
    "            axes[0].set_xlabel(\"Índice\")\n",
    "            axes[0].set_ylabel(\"Valor\")\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "            # Subplot da versão ótima\n",
    "            axes[1].plot(arr_opt)\n",
    "            axes[1].set_title(\"Opt (ótimo)\")\n",
    "            axes[1].set_xlabel(\"Índice\")\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def _ensure_batch_bound(self, b, B):\n",
    "        \"\"\"Ensure bound is (B,m).\"\"\"\n",
    "        if b is None:\n",
    "            return None\n",
    "        if b.dim() == 1:\n",
    "            return b.unsqueeze(0).expand(B, -1)\n",
    "        elif b.dim() == 2:\n",
    "            return b\n",
    "        else:\n",
    "            raise ValueError(\"bound must be (m,) or (B,m).\")\n",
    "\n",
    "    def augmented_lagrangian(self, z, lam_inf, lam_sup, lower_bound, upper_bound, pred=False):\n",
    "        \"\"\"\n",
    "        z:        (B,n)\n",
    "        lam_inf:  (B,m)\n",
    "        lam_sup:  (B,m)\n",
    "        lower_bound: (m,) or (B,m)\n",
    "        upper_bound: (m,) or (B,m)\n",
    "\n",
    "        output: (B,) augmented Lagrangian value\n",
    "        \"\"\"\n",
    "        dtype = z.dtype\n",
    "        P = self.P.to(dtype)\n",
    "        q = self.q.to(dtype).squeeze(-1)   # (n,)\n",
    "        D = self.D.to(dtype)\n",
    "\n",
    "        B = z.shape[0]\n",
    "        lb = self._ensure_batch_bound(lower_bound, B)  # (B,m)\n",
    "        ub = self._ensure_batch_bound(upper_bound, B)  # (B,m)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # QUADRATIC: 0.5 zᵀ P z\n",
    "        # ---------------------------------------------------------\n",
    "        zP = torch.matmul(z, P)                   # (B,n)\n",
    "        quad = 0.5 * torch.sum(zP * z, dim=1)     # (B,)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # LINEAR: qᵀ z\n",
    "        # ---------------------------------------------------------\n",
    "        lin = torch.sum(q * z, dim=1)             # (B,)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # D z\n",
    "        # ---------------------------------------------------------\n",
    "        Dz = torch.matmul(z, D.t())               # (B,m)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Lagrangian linear terms\n",
    "        # λ_infᵀ (l - Dz)\n",
    "        # λ_supᵀ (Dz - u)\n",
    "        # ---------------------------------------------------------\n",
    "        lag_inf = torch.sum(lam_inf * (Dz - lb), dim=1)\n",
    "        lag_sup = torch.sum(lam_sup * (ub - Dz), dim=1)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Augmented terms: ρ/2 ||max(0, l - Dz)||²\n",
    "        #                  ρ/2 ||max(0, Dz - u)||²\n",
    "        # ---------------------------------------------------------\n",
    "        viol_inf = torch.relu(lb - Dz)            # (B,m)\n",
    "        viol_sup = torch.relu(Dz - ub)            # (B,m)\n",
    "\n",
    "        aug_inf = 0.5 * self.rho * torch.sum(viol_inf ** 2, dim=1)\n",
    "        aug_sup = 0.5 * self.rho * torch.sum(viol_sup ** 2, dim=1)\n",
    "\n",
    "        if pred:\n",
    "            self.quadratic_list.append(quad.mean().item())\n",
    "            self.linear_list.append(lin.mean().item())\n",
    "            self.lambda_list.append(lag_inf.mean().item())\n",
    "            self.nu_list.append(lag_sup.mean().item())\n",
    "            self.aug_inf_list.append(aug_inf.mean().item())\n",
    "            self.aug_sup_list.append(aug_sup.mean().item())\n",
    "        else:\n",
    "            self.quadratic_list_opt.append(quad.mean().item())\n",
    "            self.linear_list_opt.append(lin.mean().item())\n",
    "            self.lambda_list_opt.append(lag_inf.mean().item())\n",
    "            self.nu_list_opt.append(lag_sup.mean().item())\n",
    "            self.aug_inf_list_opt.append(aug_inf.mean().item())\n",
    "            self.aug_sup_list_opt.append(aug_sup.mean().item())\n",
    "\n",
    "\n",
    "        return quad + lin + lag_inf + lag_sup + aug_inf + aug_sup\n",
    "    \n",
    "    def augmented_lagrangian_parsed(self, z, lam_inf, lam_sup, lower_bound, upper_bound):\n",
    "        \"\"\"\n",
    "        z:        (B,n)\n",
    "        lam_inf:  (B,m)\n",
    "        lam_sup:  (B,m)\n",
    "        lower_bound: (m,) or (B,m)\n",
    "        upper_bound: (m,) or (B,m)\n",
    "\n",
    "        output: (B,) augmented Lagrangian value\n",
    "        \"\"\"\n",
    "        dtype = z.dtype\n",
    "        P = self.P.to(dtype)\n",
    "        q = self.q.to(dtype).squeeze(-1)   # (n,)\n",
    "        D = self.D.to(dtype)\n",
    "\n",
    "        B = z.shape[0]\n",
    "        lb = self._ensure_batch_bound(lower_bound, B)  # (B,m)\n",
    "        ub = self._ensure_batch_bound(upper_bound, B)  # (B,m)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # QUADRATIC: 0.5 zᵀ P z\n",
    "        # ---------------------------------------------------------\n",
    "        zP = torch.matmul(z, P)                   # (B,n)\n",
    "        quad = 0.5 * torch.sum(zP * z, dim=1)     # (B,)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # LINEAR: qᵀ z\n",
    "        # ---------------------------------------------------------\n",
    "        lin = torch.sum(q * z, dim=1)             # (B,)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # D z\n",
    "        # ---------------------------------------------------------\n",
    "        Dz = torch.matmul(z, D.t())               # (B,m)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Lagrangian linear terms\n",
    "        # λ_infᵀ (l - Dz)\n",
    "        # λ_supᵀ (Dz - u)\n",
    "        # ---------------------------------------------------------\n",
    "        lag_inf = torch.sum(lam_inf * (lb - Dz), dim=1)\n",
    "        lag_sup = torch.sum(lam_sup * (Dz - ub), dim=1)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Augmented terms: ρ/2 ||max(0, l - Dz)||²\n",
    "        #                  ρ/2 ||max(0, Dz - u)||²\n",
    "        # ---------------------------------------------------------\n",
    "        viol_inf = torch.relu(lb - Dz)            # (B,m)\n",
    "        viol_sup = torch.relu(Dz - ub)            # (B,m)\n",
    "\n",
    "        aug_inf = 0.5 * self.rho * torch.sum(viol_inf ** 2, dim=1)\n",
    "        aug_sup = 0.5 * self.rho * torch.sum(viol_sup ** 2, dim=1)\n",
    "\n",
    "        return quad, lin, lag_inf/1e4, lag_sup/1e4, aug_inf/1e7, aug_sup/1e7\n",
    "\n",
    "    def forward(self, z_pred, z_opt, lam_inf, lam_sup, lower_bound, upper_bound):\n",
    "        \"\"\"\n",
    "        Loss = mean( (L_aug(z_pred) - L_aug(z_opt))² )\n",
    "        \"\"\"\n",
    "        L_pred = self.augmented_lagrangian(z_pred, lam_inf, lam_sup,\n",
    "                                           lower_bound, upper_bound, True)\n",
    "        L_opt = self.augmented_lagrangian(z_opt, lam_inf, lam_sup,\n",
    "                                          lower_bound, upper_bound)\n",
    "        \n",
    "\n",
    "        return ((L_pred - L_opt) ** 2).mean()\n",
    "    \n",
    "    def foward_parsed(self, z_pred, z_opt, lam_inf, lam_sup, lower_bound, upper_bound):\n",
    "        \"\"\"\n",
    "        Loss = mean( (L_aug(z_pred) - L_aug(z_opt))² )\n",
    "        \"\"\"\n",
    "        q_1, l_1, lag_min_1, lag_max_1, aug_min_1, aug_max_1 = \\\n",
    "            self.augmented_lagrangian_parsed(\n",
    "                z_pred, lam_inf, lam_sup, lower_bound, upper_bound\n",
    "            )\n",
    "\n",
    "        q_2, l_2, lag_min_2, lag_max_2, aug_min_2, aug_max_2 = \\\n",
    "            self.augmented_lagrangian_parsed(\n",
    "                z_opt, lam_inf, lam_sup, lower_bound, upper_bound\n",
    "            )\n",
    "        \n",
    "        print(f\"LAG PRED: {q_1 + l_1 + lag_min_1 + lag_max_1 + aug_min_1 + aug_max_1}\")\n",
    "        print(f\"LAG OPT: {q_2 + l_2 + lag_min_2 + lag_max_2 + aug_min_2 + aug_max_2}\")\n",
    "\n",
    "        def mse(x, y):\n",
    "            return ((x - y) ** 2).mean()\n",
    "\n",
    "        return {\n",
    "            \"mse_q\": mse(q_1, q_2).item(),\n",
    "            \"mse_l\": mse(l_1, l_2).item(),\n",
    "            \"mse_lag_min\": mse(lag_min_1, lag_min_2).item(),\n",
    "            \"mse_lag_max\": mse(lag_max_1, lag_max_2).item(),\n",
    "            \"mse_aug_min\": mse(aug_min_1, aug_min_2).item(),\n",
    "            \"mse_aug_max\": mse(aug_max_1, aug_max_2).item(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec312d",
   "metadata": {},
   "source": [
    "## 3.2 Loss Final - Primal e Restrições\n",
    "\n",
    "Tentando emular o comportamento da função anterior, que é sensível à otimalidade e à factibilidade da solução, sem cálculos complexos que podem introduzir instabilidade e complexidade, foi construída essa segunda função:\n",
    "$$\n",
    "\\ell(\\theta) :=\n",
    "\\sum_{i=1}^{}\n",
    "\\Big(\n",
    "    (\\tilde{\\pi}(x_i \\mid \\theta) - z^*) + \\max{(0, z_{inf} - \\mathbf{D}z)} + \\max{(0, \\mathbf{D}z - z_{sup})}\n",
    "\\Big)^2\n",
    "$$\n",
    "\n",
    "- A diferença entre o primal estimado e o ótimo contribui para o atendimento do requisito de otimalidade\n",
    "- A incorporação dos termos do lagrangiano aumentado ajudam a penalizar resultados que violam as restrições do problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPLoss(nn.Module):\n",
    "    def __init__(self, D, lam=1e-4):\n",
    "        \"\"\"\n",
    "        D : matriz de restrições (m, n) — fixa para todo o dataset\n",
    "        lam : peso da penalização das violações\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lam = lam\n",
    "        \n",
    "        self.register_buffer(\"D\", D)\n",
    "\n",
    "        self.primal_list = []\n",
    "        self.penalty_list = []\n",
    "\n",
    "    def visualize(self, show_total=True):\n",
    "        \"\"\"\n",
    "        Plota a evolução dos termos da loss ao longo do treinamento.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.primal_list) == 0:\n",
    "            print(\"Nenhum valor armazenado. Chame forward(..., store=True) durante o treino.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.plot(self.primal_list, label=\"Primal (MSE)\", linewidth=2)\n",
    "        plt.plot(self.penalty_list, label=f\"Penalty (λ * restrições)\", linewidth=2)\n",
    "\n",
    "        if show_total:\n",
    "            total = [p + q for p, q in zip(self.primal_list, self.penalty_list)]\n",
    "            plt.plot(total, label=\"Total Loss\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "        plt.xlabel(\"Iterações de Treino\")\n",
    "        plt.ylabel(\"Valores\")\n",
    "        plt.title(\"Evolução dos termos da Loss QP\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def forward(self, z_pred, z_star, a, b, store=False):\n",
    "\n",
    "        # 1. MSE\n",
    "        mse = torch.mean((z_pred - z_star)**2)\n",
    "\n",
    "        # 2. D no mesmo device que z_pred\n",
    "        D = self.D.to(z_pred.device)\n",
    "        Dz = z_pred @ D.T\n",
    "\n",
    "        viol_sup = torch.relu(Dz - b)\n",
    "        viol_inf = torch.relu(a - Dz)\n",
    "\n",
    "        penalty = torch.mean(viol_sup**2 + viol_inf**2)\n",
    "\n",
    "        loss = mse + self.lam * penalty\n",
    "\n",
    "        if store:\n",
    "            self.primal_list.append(mse.item())\n",
    "            self.penalty_list.append((self.lam * penalty).item())\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb891f",
   "metadata": {},
   "source": [
    "### 3.2.1 Validação da Função de Perda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b537cec",
   "metadata": {},
   "source": [
    "#### 3.2.1.1 Distribuição de Probabilidade para Valores Aleatórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MPC_DATASET(MPC)\n",
    "criterion = QPLoss(dataset.D, lam=1e-4).to(DEVICE)\n",
    "\n",
    "def sample_random_z(batch, n, scale=1.0):\n",
    "    return scale * torch.randn(batch, n)\n",
    "\n",
    "loss_values = []\n",
    "N = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, N):\n",
    "\n",
    "        print(f\"\\n########### SAMPLE {i} ############\")\n",
    "\n",
    "        sample = dataset[i]\n",
    "        z_opt = sample[\"z\"].to(DEVICE)\n",
    "        lb    = sample[\"lb\"].to(DEVICE)\n",
    "        ub    = sample[\"ub\"].to(DEVICE)\n",
    "\n",
    "        z_opt = z_opt.unsqueeze(0)\n",
    "        lb    = lb.unsqueeze(0)\n",
    "        ub    = ub.unsqueeze(0)\n",
    "\n",
    "        z_rand = sample_random_z(batch=64, n=z_opt.shape[1]).to(DEVICE)\n",
    "\n",
    "        print(f\"[z_pred exemplo]: {z_rand[0, :5].cpu().numpy()} ...\")\n",
    "        print(f\"[z_opt exemplo]:  {z_opt[0, :5].cpu().numpy()} ...\")\n",
    "\n",
    "        loss = criterion(z_rand, z_opt, lb, ub, store=True)\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "plt.hist(loss_values, bins=40)\n",
    "plt.title(f\"Distribuição da loss para z aleatórios (N instâncias)\")\n",
    "plt.show()\n",
    "\n",
    "criterion.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0dae1",
   "metadata": {},
   "source": [
    "#### 3.2.1.2 Ponto Ótimo como Mínimo Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_zero_at_optimum(criterion, dataset, device):\n",
    "    zero_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            sample = dataset[i]\n",
    "            z_opt = sample[\"z\"].to(device).unsqueeze(0)\n",
    "            lb    = sample[\"lb\"].to(device).unsqueeze(0)\n",
    "            ub    = sample[\"ub\"].to(device).unsqueeze(0)\n",
    "\n",
    "            # loss no ótimo\n",
    "            loss = criterion(z_opt, z_opt, lb, ub).item()\n",
    "            zero_losses.append(loss)\n",
    "\n",
    "    print(\"\\n=== Teste: Loss no ótimo ===\")\n",
    "    print(f\"Média  : {sum(zero_losses)/len(zero_losses):.6e}\")\n",
    "    print(f\"Máximo : {max(zero_losses):.6e}\")\n",
    "    print(f\"Mínimo : {min(zero_losses):.6e}\")\n",
    "\n",
    "    plt.hist(zero_losses, bins=40)\n",
    "    plt.title(\"Loss(z_opt, z_opt) — Deve ser zero\")\n",
    "    plt.show()\n",
    "\n",
    "dataset = MPC_DATASET(MPC)\n",
    "criterion = QPLoss(dataset.D).to(DEVICE)\n",
    "test_loss_zero_at_optimum(criterion, dataset, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59f363",
   "metadata": {},
   "source": [
    "#### 3.2.1.3 Validação da Curvatura em torno do ótimo para garantir ausência de mínimos locais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_local_curvature(criterion, dataset, device, epsilons=None):\n",
    "    if epsilons is None:\n",
    "        epsilons = [1e-3, 1e-2, 1e-1, 1.0]\n",
    "\n",
    "    losses_per_eps = {eps: [] for eps in epsilons}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(10000):\n",
    "            sample = dataset[i]\n",
    "            z_opt = sample[\"z\"].to(device).unsqueeze(0)\n",
    "            lb    = sample[\"lb\"].to(device).unsqueeze(0)\n",
    "            ub    = sample[\"ub\"].to(device).unsqueeze(0)\n",
    "\n",
    "            n = z_opt.shape[1]\n",
    "\n",
    "            direction = torch.randn(1, n).to(device)\n",
    "            direction = direction / (torch.norm(direction) + 1e-9)\n",
    "\n",
    "            for eps in epsilons:\n",
    "                z_test = z_opt + eps * direction\n",
    "                loss = criterion(z_test, z_opt, lb, ub).item()\n",
    "                losses_per_eps[eps].append(loss)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7,5))\n",
    "    mean_losses = []\n",
    "\n",
    "    for eps in epsilons:\n",
    "        mean_loss = sum(losses_per_eps[eps]) / len(losses_per_eps[eps])\n",
    "        mean_losses.append(mean_loss)\n",
    "        plt.plot([eps], [mean_loss], \"o\", label=f\"eps={eps}, mean={mean_loss:.2e}\")\n",
    "\n",
    "    plt.plot(epsilons, mean_losses, \"-k\", linewidth=1)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Perturbação ε (log scale)\")\n",
    "    plt.ylabel(\"Loss média (log scale)\")\n",
    "    plt.title(\"Monotonicidade local da Loss ao se afastar do primal\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataset = MPC_DATASET(MPC)\n",
    "criterion = QPLoss(dataset.D).to(DEVICE)\n",
    "test_local_curvature(criterion, dataset, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08583a",
   "metadata": {},
   "source": [
    "#### 3.2.1.4 Estabilidade da Norma do Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient_explosion(criterion, dataset, device, sigmas=None):\n",
    "    if sigmas is None:\n",
    "        sigmas = [1e-2, 1e-1, 1, 10, 100]\n",
    "\n",
    "    grad_norms_by_sigma = {s: [] for s in sigmas}\n",
    "\n",
    "    for i in range(1000):\n",
    "        sample = dataset[i]\n",
    "        z_opt = sample[\"z\"].to(device).unsqueeze(0)\n",
    "        lb    = sample[\"lb\"].to(device).unsqueeze(0)\n",
    "        ub    = sample[\"ub\"].to(device).unsqueeze(0)\n",
    "\n",
    "        n = z_opt.shape[1]\n",
    "\n",
    "        for sigma in sigmas:\n",
    "            # cria z_pred aleatório com variância controlada\n",
    "            z_pred = (sigma * torch.randn(1, n)).to(device)\n",
    "            z_pred.requires_grad_(True)\n",
    "\n",
    "            loss = criterion(z_pred, z_opt, lb, ub)\n",
    "            loss.backward()\n",
    "\n",
    "            grad = z_pred.grad.detach()\n",
    "            grad_norm = torch.norm(grad).item()\n",
    "\n",
    "            grad_norms_by_sigma[sigma].append(grad_norm)\n",
    "\n",
    "    # ---- Plot ----\n",
    "    plt.figure(figsize=(7,5))\n",
    "\n",
    "    means = []\n",
    "    for sigma in sigmas:\n",
    "        mean_norm = sum(grad_norms_by_sigma[sigma]) / len(grad_norms_by_sigma[sigma])\n",
    "        means.append(mean_norm)\n",
    "        plt.plot(sigma, mean_norm, \"o\", label=f\"sigma={sigma}, mean={mean_norm:.2e}\")\n",
    "\n",
    "    plt.plot(sigmas, means, \"-k\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Escala do z_pred aleatório (sigma)\")\n",
    "    plt.ylabel(\"Norma média do gradiente\")\n",
    "    plt.title(\"Exploding Gradient Test da QPLoss\")\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "dataset = MPC_DATASET(MPC)\n",
    "criterion = QPLoss(dataset.D).to(DEVICE)\n",
    "test_gradient_explosion(criterion, dataset, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc895c",
   "metadata": {},
   "source": [
    "# 4. Definição da Rede e Exploração\n",
    "\n",
    "Construção da rede básica conforme as especificações do artigo, com a dimensionalidade das camadas ocultas parametrizada.\n",
    "\n",
    "- Função de Ativação: ReLU - indicada no paper para a aproximação de problemas QP Convexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20677cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden=(64, 64)):\n",
    "\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "\n",
    "        for h in hidden:\n",
    "            linear = nn.Linear(prev, h)\n",
    "            nn.init.xavier_uniform_(linear.weight,\n",
    "                                    gain=nn.init.calculate_gain(\"relu\"))\n",
    "            nn.init.zeros_(linear.bias)\n",
    "            layers += [linear, nn.ReLU()]\n",
    "            prev = h\n",
    "\n",
    "        out = nn.Linear(prev, output_dim)\n",
    "        nn.init.xavier_uniform_(out.weight,\n",
    "                                gain=1.0)\n",
    "        nn.init.zeros_(out.bias)\n",
    "\n",
    "        layers.append(out)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_std):\n",
    "        return self.net(x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f3552",
   "metadata": {},
   "source": [
    "## 4.1 Treinamento Exploratório\n",
    "\n",
    "Para validação do modelo, é realizado um treinamento base para explorar o comportamento da função de perda e do gradiente, com o objetivo de verificar a estabilidade para o treinamento final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd510ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_norm(model):\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total += p.grad.detach().norm(2).item()**2\n",
    "    return total**0.5\n",
    "\n",
    "\n",
    "def grad_direction(model, prev_grad):\n",
    "    g = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            g.append(p.grad.detach().flatten())\n",
    "    g = torch.cat(g)\n",
    "\n",
    "    if prev_grad is None:\n",
    "        return None, g\n",
    "\n",
    "    cos = torch.nn.functional.cosine_similarity(g, prev_grad, dim=0).item()\n",
    "    return cos, g\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, loss_fn, loader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            z = batch[\"z\"].to(device)\n",
    "            lb = batch[\"lb\"].to(device)\n",
    "            ub = batch[\"ub\"].to(device)\n",
    "\n",
    "            z_pred = model(x)\n",
    "            loss = loss_fn(z_pred, z, lb, ub, store=False)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def plot_training_history(history):\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 14))\n",
    "\n",
    "    axs[0].plot(history[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
    "    axs[0].plot(history[\"val_loss\"], label=\"Val Loss\", linewidth=2)\n",
    "    axs[0].set_title(\"Loss no Treino e Teste\")\n",
    "    axs[0].set_xlabel(\"Época\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history[\"grad_norm\"], label=\"Norma do Gradiente\")\n",
    "    axs[1].set_title(\"Norma do Gradiente ao Longo do Tempo\")\n",
    "    axs[1].set_xlabel(\"Iteração\")\n",
    "    axs[1].set_ylabel(\"||g||\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    axs[2].plot(history[\"grad_cosine\"], label=\"Cosine Similarity\", color=\"green\")\n",
    "    axs[2].set_title(\"Direção do Gradiente (similaridade coseno)\")\n",
    "    axs[2].set_xlabel(\"Iteração\")\n",
    "    axs[2].set_ylabel(\"cos(g_t, g_{t-1})\")\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    loss_fn,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    epochs=50,\n",
    "    device=\"cpu\",\n",
    "    patience=75,\n",
    "    min_delta=1e-6\n",
    "):\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"grad_norm\": [],\n",
    "        \"grad_cosine\": []\n",
    "    }\n",
    "\n",
    "    prev_grad = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            z = batch[\"z\"].to(device)\n",
    "            lb = batch[\"lb\"].to(device)\n",
    "            ub = batch[\"ub\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            z_pred = model(x)\n",
    "            loss = loss_fn(z_pred, z, lb, ub, store=True)\n",
    "            loss.backward()\n",
    "\n",
    "            # --- Compute gradient norm ---\n",
    "            grad_vec = torch.cat([\n",
    "                p.grad.reshape(-1) for p in model.parameters() if p.grad is not None\n",
    "            ])\n",
    "            grad_norm = grad_vec.norm().item()\n",
    "            history[\"grad_norm\"].append(grad_norm)\n",
    "\n",
    "            # --- Compute gradient direction (cosine similarity) ---\n",
    "            if prev_grad is not None:\n",
    "                cosine = torch.nn.functional.cosine_similarity(\n",
    "                    grad_vec, prev_grad, dim=0\n",
    "                ).item()\n",
    "            else:\n",
    "                cosine = 1.0\n",
    "            history[\"grad_cosine\"].append(cosine)\n",
    "            prev_grad = grad_vec.detach()\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= len(train_loader)\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "\n",
    "        val_loss = evaluate_model(model, loss_fn, val_loader, device)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch+1:03d}] \"\n",
    "            f\"Train Loss = {epoch_loss:.6f} | \"\n",
    "            f\"Val Loss = {val_loss:.6f} | \"\n",
    "            f\"Grad Norm = {grad_norm:.3e} | \"\n",
    "            f\"Cosine Dir = {cosine:.3f}\"\n",
    "        )\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        if val_loss + min_delta < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\nEarly stopping ativado após {epoch+1} epochs sem melhora.\\n\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    print(\"\\nTreino finalizado! Gerando gráficos...\\n\")\n",
    "    plot_training_history(history)\n",
    "\n",
    "    print(\"\\nGerando gráficos específicos da QPLoss...\")\n",
    "    loss_fn.visualize(show_total=True)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ece12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = MPC_DATASET(MPC)\n",
    "\n",
    "train_size = int(0.8 * len(DATASET))\n",
    "val_size   = len(DATASET) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(DATASET, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,  batch_size=256, shuffle=False)\n",
    "\n",
    "model = PlannerNet(MPC.n_states, MPC.dim_z)\n",
    "loss_fn = QPLoss(DATASET.D)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc23947",
   "metadata": {},
   "source": [
    "# 5. Modelo Definitivo e Treinamento Final\n",
    "\n",
    "Para verificar a melhor possibilidade de modelos, foi implementada uma metodologia de exploração a partir de um Grid Search, definindo um espaço de possibilidade paramétrico de NN, a partir do qual cada cenário será treinado com K-Folding e avaliado conforme o menor valor esperado de função de perda no conjunto de teste.\n",
    "\n",
    "- **OBS**: Como são muitas possibilidades e o processo de treinamento é moroso, a célula de search demora muito, cerca de 16 horas no computador utilizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a7580",
   "metadata": {},
   "source": [
    "## 5.1 Funções de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, loader, optimizer, device, clip_value=None, store_loss=False):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        z = batch[\"z\"].to(device)\n",
    "        lb = batch[\"lb\"].to(device)\n",
    "        ub = batch[\"ub\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        z_pred = model(x)\n",
    "        loss = loss_fn(z_pred, z, lb, ub, store=store_loss)\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_value is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return running_loss / max(1, n_batches)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loss_fn, loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        z = batch[\"z\"].to(device)\n",
    "        lb = batch[\"lb\"].to(device)\n",
    "        ub = batch[\"ub\"].to(device)\n",
    "\n",
    "        z_pred = model(x)\n",
    "        loss = loss_fn(z_pred, z, lb, ub, store=False)\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return running_loss / max(1, n_batches)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def k_fold_train(\n",
    "    dataset,\n",
    "    model_fn: Callable[[], torch.nn.Module],\n",
    "    loss_fn_fn: Callable[[], torch.nn.Module],\n",
    "    k: int = 5,\n",
    "    num_epochs: int = 100,\n",
    "    batch_size: int = 256,\n",
    "    lr: float = 3e-4,\n",
    "    weight_decay: float = 1e-5,\n",
    "    device: str = None,\n",
    "    patience: int = 100,\n",
    "    min_delta: float = 1e-6,\n",
    "    clip_value: float = 1.0,\n",
    "    verbose: bool = True,\n",
    "    use_tqdm_folds: bool = True,\n",
    "    use_tqdm_epochs: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    if device is None:\n",
    "        device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "    n = len(dataset)\n",
    "    indices = np.arange(n)\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # Compute fold sizes\n",
    "    fold_sizes = np.full(k, n // k, dtype=int)\n",
    "    fold_sizes[: n % k] += 1\n",
    "    current = 0\n",
    "\n",
    "    all_fold_histories = []\n",
    "    best_model_states = []\n",
    "\n",
    "    # -----------------------\n",
    "    # Setup tqdm for FOLDS\n",
    "    # -----------------------\n",
    "    fold_range = range(k)\n",
    "    if use_tqdm_folds:\n",
    "        fold_range = tqdm(range(k), desc=\"K-Fold\", leave=False)\n",
    "\n",
    "    for fold in fold_range:\n",
    "\n",
    "        start, stop = current, current + fold_sizes[fold]\n",
    "        val_idx = indices[start:stop]\n",
    "        train_idx = np.setdiff1d(indices, val_idx)\n",
    "        current = stop\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx.tolist())\n",
    "        val_subset = Subset(dataset, val_idx.tolist())\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_fn().to(device)\n",
    "        loss_fn = loss_fn_fn()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        best_val = float(\"inf\")\n",
    "        best_state = None\n",
    "        patience_counter = 0\n",
    "\n",
    "        history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "        if verbose and not use_tqdm_folds:\n",
    "            print(f\"\\n=== Fold {fold+1}/{k} | train {len(train_idx)} samples | val {len(val_idx)} samples ===\")\n",
    "\n",
    "        # -----------------------\n",
    "        # tqdm for EPOCHS\n",
    "        # -----------------------\n",
    "        epoch_range = range(1, num_epochs + 1)\n",
    "        if use_tqdm_epochs:\n",
    "            epoch_range = tqdm(epoch_range, desc=f\"Fold {fold+1}\", leave=False)\n",
    "\n",
    "        for epoch in epoch_range:\n",
    "            train_loss = train_one_epoch(\n",
    "                model, loss_fn, train_loader, optimizer,\n",
    "                device, clip_value=clip_value, store_loss=True\n",
    "            )\n",
    "\n",
    "            val_loss = evaluate(model, loss_fn, val_loader, device)\n",
    "\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "            if verbose and not use_tqdm_epochs:\n",
    "                print(\n",
    "                    f\"Fold {fold+1} | Epoch {epoch:03d} \"\n",
    "                    f\"| Train {train_loss:.6f} | Val {val_loss:.6f} | Best {best_val:.6f}\"\n",
    "                )\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss + min_delta < best_val:\n",
    "                best_val = val_loss\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping on fold {fold+1} at epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "        all_fold_histories.append(history)\n",
    "        best_model_states.append({\n",
    "            \"fold\": fold,\n",
    "            \"best_val_loss\": best_val,\n",
    "            \"state_dict\": best_state\n",
    "        })\n",
    "\n",
    "    result = {\n",
    "        \"fold_histories\": all_fold_histories,\n",
    "        \"best_models\": best_model_states,\n",
    "        \"params\": {\n",
    "            \"k\": k,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"patience\": patience,\n",
    "            \"clip_value\": clip_value\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5daf724",
   "metadata": {},
   "source": [
    "## 5.2 Funções de Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_param_combinations(param_grid):\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    for combo in product(*values):\n",
    "        params = dict(zip(keys, combo))\n",
    "        yield params\n",
    "        \n",
    "def grid_search(param_grid, k=5, num_epochs=300):\n",
    "\n",
    "    all_results = []\n",
    "    best_result = None\n",
    "    best_avg_val_loss = float(\"inf\")\n",
    "\n",
    "    def mean_best_val_loss(result):\n",
    "        fold_histories = result[\"fold_histories\"]\n",
    "        best_vals = [min(f[\"val_loss\"]) for f in fold_histories]\n",
    "        return sum(best_vals) / len(best_vals)\n",
    "\n",
    "    param_combinations = list(generate_param_combinations(param_grid))\n",
    "\n",
    "    print(f\"\\n=== Iniciando Grid Search com {len(param_combinations)} combinações ===\\n\")\n",
    "\n",
    "    for params in tqdm(param_combinations, desc=\"Grid Search\"):\n",
    "\n",
    "        dataset = MPC_DATASET(MPC)\n",
    "\n",
    "        result = k_fold_train(\n",
    "            dataset=dataset,\n",
    "            model_fn=lambda : PlannerNet(\n",
    "                MPC.n_states,\n",
    "                MPC.dim_z,\n",
    "                (params[\"hidden_dim\"], params[\"hidden_dim\"])\n",
    "            ),\n",
    "            loss_fn_fn=lambda : QPLoss(dataset.D),\n",
    "            k=k,\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            lr=params[\"lr\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "            patience=75,\n",
    "            min_delta=1e-6,\n",
    "            clip_value=1.0,\n",
    "            verbose=True,\n",
    "            use_tqdm_folds=False\n",
    "        )\n",
    "\n",
    "        avg_val = mean_best_val_loss(result)\n",
    "\n",
    "        tqdm.write(f\"Params={params} | Média da Val Loss={avg_val:.6f}\")\n",
    "\n",
    "        entry = {\"params\": params, \"result\": result}\n",
    "        all_results.append(entry)\n",
    "\n",
    "        if avg_val < best_avg_val_loss:\n",
    "            best_avg_val_loss = avg_val\n",
    "            best_result = entry\n",
    "\n",
    "    return {\n",
    "        \"all_results\": all_results,\n",
    "        \"best_result\": best_result,\n",
    "        \"best_val_loss\": best_avg_val_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54abb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"hidden_dim\": [32, 64, 128],\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4],\n",
    "    \"batch_size\": [32, 64, 128],\n",
    "    \"weight_decay\": [0.0, 1e-5],\n",
    "}\n",
    "\n",
    "grid_search_result = grid_search(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759f06f",
   "metadata": {},
   "source": [
    "## 5.3 Treinamento Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa718e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search_result[\"best_result\"][\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d058ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = MPC_DATASET(MPC)\n",
    "\n",
    "# params = grid_search_result[\"best_result\"][\"params\"]\n",
    "params = {'hidden_dim': 128, 'lr': 0.001, 'batch_size': 64, 'weight_decay': 1e-05}\n",
    "\n",
    "train_size = int(0.8 * len(DATASET))\n",
    "val_size   = len(DATASET) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(DATASET, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,  batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "model = PlannerNet(MPC.n_states, MPC.dim_z, (params[\"hidden_dim\"], params[\"hidden_dim\"]))\n",
    "loss_fn = QPLoss(DATASET.D)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=300,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddeb214",
   "metadata": {},
   "source": [
    "## 5.4 Resultados Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a582a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_loss = 0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        x = batch[\"x\"].to(DEVICE)\n",
    "        z = batch[\"z\"].to(DEVICE)   # alvo\n",
    "        # Se houver limites:\n",
    "        lb = batch[\"lb\"].to(DEVICE)\n",
    "        ub = batch[\"ub\"].to(DEVICE)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, z, lb, ub)  # adapte para sua loss\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        all_preds.append(y_pred.cpu().numpy())\n",
    "        all_targets.append(z.cpu().numpy())\n",
    "\n",
    "val_loss /= len(val_dataset)\n",
    "print(f\"Mean Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "mse = mean_squared_error(all_targets, all_preds)\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "r2  = r2_score(all_targets, all_preds)\n",
    "\n",
    "errors = all_preds - all_targets\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(errors)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Prediction error\")\n",
    "plt.title(\"Prediction Errors on Validation Set\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(all_targets, all_preds, alpha=0.5)\n",
    "plt.plot([all_targets.min(), all_targets.max()],\n",
    "         [all_targets.min(), all_targets.max()], 'r--')\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.title(\"Predictions vs Targets\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "errors = all_preds - all_targets\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(errors, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"Prediction error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Prediction Errors\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa873be",
   "metadata": {},
   "source": [
    "# 6. Exportando Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a54e3",
   "metadata": {},
   "source": [
    "## 6.1 Modelo Cru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8152c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MPC_DATASET(MPC)\n",
    "sample = dataset[0]\n",
    "init_state_sample = sample[\"x\"].unsqueeze(0)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    init_state_sample,\n",
    "    \"planner_raw.onnx\",\n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"z_pred\"],\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8cde9",
   "metadata": {},
   "source": [
    "## 6.2 Modelo Quantizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model = \"planner_raw.onnx\"\n",
    "output_model = \"planner_quant.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=input_model,\n",
    "    model_output=output_model,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    optimize_model=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
